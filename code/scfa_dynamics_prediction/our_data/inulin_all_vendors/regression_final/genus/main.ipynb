{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liaoc/opt/anaconda3/lib/python3.8/site-packages/umap/__init__.py:9: UserWarning: Tensorflow not installed; ParametricUMAP will be unavailable\n",
      "  warn(\"Tensorflow not installed; ParametricUMAP will be unavailable\")\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import make_pipeline                                           \n",
    "from sklearn.preprocessing import StandardScaler                                     \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.base import clone\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import distance\n",
    "from skbio.stats.ordination import pcoa\n",
    "import umap\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import linear_model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read meta data\n",
    "df_meta = pd.read_csv('../../../../../../our_data/meta_data.csv', index_col=0)\n",
    "df_meta = df_meta[df_meta.Diet=='Inulin'] # only for inulin group\n",
    "df_meta = df_meta[df_meta.Day != 0] # remove day 0\n",
    "\n",
    "# read SCFA data\n",
    "df_scfa = pd.read_csv('../../../../../../our_data/SCFA.csv', index_col=0)\n",
    "\n",
    "# read bacterial abundance (genus level)\n",
    "df_bac = pd.read_csv('../../../../../../our_data/16S_absolute_abundance_genus.csv', index_col=0)\n",
    "\n",
    "# find common samples\n",
    "common_samples = list(set(df_meta.index).intersection(df_scfa.index).intersection(df_bac.index))\n",
    "df_meta = df_meta.loc[common_samples]\n",
    "df_scfa = df_scfa.loc[common_samples]\n",
    "df_bac = df_bac.loc[common_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self-defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(X, Z, method=None): # X is test and Z is train\n",
    "    X.loc[:, \"is_z\"] = 0\n",
    "    Z.loc[:, \"is_z\"] = 1\n",
    "    XZ = pd.concat([X, Z],ignore_index=True) # keep index\n",
    "    labels = XZ['is_z'].values\n",
    "    XZ_mat = XZ.drop('is_z', axis=1).values\n",
    "     \n",
    "    # test if X and Z can be distinguished\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=3,random_state=0)\n",
    "    predictions = np.zeros(labels.shape)\n",
    "    skf = SKF(n_splits=20, shuffle=True, random_state=0)\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(XZ_mat, labels)):\n",
    "        X_train, X_test = XZ_mat[train_idx,:], XZ_mat[test_idx,:]\n",
    "        y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        probs = clf.predict_proba(X_test)[:, 1] # probability that each data point is a sample from training set\n",
    "        predictions[test_idx] = probs\n",
    "    roc_auc = AUC(labels, predictions)\n",
    "    \n",
    "    # weight_i = p_i(X|D)/p_i(Z|D)\n",
    "    predictions_Z = predictions[len(X):] # p(Z/D)\n",
    "    weights = (1./predictions_Z) - 1. # p(X|D)/p(Z/D)\n",
    "    weights /= np.mean(weights) # we do this to re-normalize the computed log-loss\n",
    "    XZ['size'] = 4\n",
    "    XZ.iloc[len(X):, XZ.columns.get_loc('size')] = 0.1 + weights*15\n",
    "    \n",
    "    # perform decomposition\n",
    "    if method is not None:\n",
    "        XZ = XZ.sort_index()\n",
    "        XZ_mat = XZ.drop(['is_z','size'], axis=1).values\n",
    "\n",
    "        # dimensionality reduction\n",
    "        if method=='UMAP':\n",
    "            fit = umap.UMAP(random_state=0)\n",
    "            u = fit.fit_transform(XZ_mat)\n",
    "            XZ_dec = pd.DataFrame(u, index=XZ.index, columns=['Axis1','Axis2'])\n",
    "        elif method=='PCoA':\n",
    "            dist_relab = distance.squareform(distance.pdist(XZ_mat, metric=\"braycurtis\"))\n",
    "            OrdinationResults = pcoa(dist_relab, number_of_dimensions=2)\n",
    "            XZ_dec = pd.DataFrame(OrdinationResults.samples.values, index=XZ.index, columns=['Axis1','Axis2'])\n",
    "        else:\n",
    "            print('uknown method: %s'%(method))\n",
    "            raise\n",
    "\n",
    "        XZ_dec['is_z'] = XZ['is_z']\n",
    "        XZ_dec['size'] = XZ['size']        \n",
    "        return roc_auc, weights, XZ_dec\n",
    "    else:\n",
    "        return roc_auc, weights, XZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = {}\n",
    "results = []\n",
    "use_weights=False\n",
    "plot_weights=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intrapolation, group A, Acetate, best score and parameter combination = \n",
      "0.16101570966223486\n",
      "{'randomforestregressor__max_depth': 16, 'randomforestregressor__max_features': 0.32, 'randomforestregressor__min_samples_leaf': 1, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.1}\n",
      "\n",
      "\n",
      "Intrapolation, group A, Propionate, best score and parameter combination = \n",
      "0.4062991997296647\n",
      "{'randomforestregressor__max_depth': 8, 'randomforestregressor__max_features': 'auto', 'randomforestregressor__min_samples_leaf': 4, 'randomforestregressor__min_samples_split': 16, 'selectfrommodel__estimator__alpha': 0.1}\n",
      "\n",
      "\n",
      "Intrapolation, group A, Butyrate, best score and parameter combination = \n",
      "0.46754780750768266\n",
      "{'randomforestregressor__max_depth': 16, 'randomforestregressor__max_features': 0.64, 'randomforestregressor__min_samples_leaf': 2, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.1}\n",
      "\n",
      "\n",
      "Intrapolation, group B, Acetate, best score and parameter combination = \n",
      "0.12651440816026832\n",
      "{'randomforestregressor__max_depth': 4, 'randomforestregressor__max_features': 0.32, 'randomforestregressor__min_samples_leaf': 1, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.1}\n",
      "\n",
      "\n",
      "Intrapolation, group B, Propionate, best score and parameter combination = \n",
      "0.37463019915412454\n",
      "{'randomforestregressor__max_depth': 16, 'randomforestregressor__max_features': 0.32, 'randomforestregressor__min_samples_leaf': 4, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.001}\n",
      "\n",
      "\n",
      "Intrapolation, group B, Butyrate, best score and parameter combination = \n",
      "0.2936003906621984\n",
      "{'randomforestregressor__max_depth': 4, 'randomforestregressor__max_features': 0.64, 'randomforestregressor__min_samples_leaf': 2, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.01}\n",
      "\n",
      "\n",
      "Intrapolation, group C, Acetate, best score and parameter combination = \n",
      "0.12001439884064583\n",
      "{'randomforestregressor__max_depth': 8, 'randomforestregressor__max_features': 0.32, 'randomforestregressor__min_samples_leaf': 2, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.0001}\n",
      "\n",
      "\n",
      "Intrapolation, group C, Propionate, best score and parameter combination = \n",
      "0.23656482316090627\n",
      "{'randomforestregressor__max_depth': 8, 'randomforestregressor__max_features': 'log2', 'randomforestregressor__min_samples_leaf': 2, 'randomforestregressor__min_samples_split': 8, 'selectfrommodel__estimator__alpha': 0.1}\n",
      "\n",
      "\n",
      "Intrapolation, group C, Butyrate, best score and parameter combination = \n",
      "0.2539576064738246\n",
      "{'randomforestregressor__max_depth': 8, 'randomforestregressor__max_features': 0.64, 'randomforestregressor__min_samples_leaf': 2, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.001}\n",
      "\n",
      "\n",
      "Intrapolation, group D, Acetate, best score and parameter combination = \n",
      "0.05758545667700872\n",
      "{'randomforestregressor__max_depth': 16, 'randomforestregressor__max_features': 'log2', 'randomforestregressor__min_samples_leaf': 2, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.0001}\n",
      "\n",
      "\n",
      "Intrapolation, group D, Propionate, best score and parameter combination = \n",
      "0.41929828918334816\n",
      "{'randomforestregressor__max_depth': 16, 'randomforestregressor__max_features': 0.32, 'randomforestregressor__min_samples_leaf': 4, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.01}\n",
      "\n",
      "\n",
      "Intrapolation, group D, Butyrate, best score and parameter combination = \n",
      "0.25883195996089214\n",
      "{'randomforestregressor__max_depth': 16, 'randomforestregressor__max_features': 'auto', 'randomforestregressor__min_samples_leaf': 4, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.01}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if use_weights and plot_weights:\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(20,4), sharex=True, sharey=True)\n",
    "\n",
    "for k,group_to_exclude in enumerate(['A','B','C','D']):\n",
    "\n",
    "    # split train/test data\n",
    "    mice_to_keep = list(set(df_meta[df_meta.RandomizedGroup!=group_to_exclude].MiceID))\n",
    "    samples_to_keep = list(set(df_meta[df_meta.MiceID.isin(mice_to_keep)].index))\n",
    "    mice_to_exclude = list(set(df_meta[df_meta.RandomizedGroup==group_to_exclude].MiceID))\n",
    "    samples_to_exclude = list(set(df_meta[df_meta.MiceID.isin(mice_to_exclude)].index))\n",
    "\n",
    "    # get weights of training sets\n",
    "    xdata_train = df_bac.loc[samples_to_keep]\n",
    "    xdata_test = df_bac.loc[samples_to_exclude]\n",
    "    if use_weights:        \n",
    "        roc_auc[group_to_exclude], weights, df_ord = get_weights(X=xdata_test, Z=xdata_train, method='PCoA')\n",
    "        assert len(xdata_train)==len(weights)\n",
    "        print (\"ROC-AUC (%s): %2.2f\" % (group_to_exclude, roc_auc[group_to_exclude]))\n",
    "     \n",
    "        # show weights on train and test\n",
    "        if plot_weights:\n",
    "            # _ = sns.scatterplot(x='Axis1', y='Axis2', hue='is_z', size='size', data=df_ord, ax=ax[k])\n",
    "            df_ord_0 = df_ord.loc[df_ord.is_z==0]\n",
    "            _ = ax[k].scatter(df_ord_0['Axis1'], df_ord_0['Axis2'], marker='o', s=df_ord_0['size'], c='r', label='test')\n",
    "            df_ord_1 = df_ord.loc[df_ord.is_z==1]\n",
    "            _ = ax[k].scatter(df_ord_1['Axis1'], df_ord_1['Axis2'], marker='o', s=df_ord_1['size'], c='b', label='train')\n",
    "\n",
    "    xdata_train = np.asarray(xdata_train.values)\n",
    "    xdata_test = np.asarray(xdata_test.values)\n",
    "    \n",
    "    # run random forest regression with weights\n",
    "    for scfa in ['Acetate','Propionate','Butyrate']:                \n",
    "        ydata_train = np.asarray(df_scfa.loc[samples_to_keep, scfa])\n",
    "        ydata_test = np.asarray(df_scfa.loc[samples_to_exclude, scfa])\n",
    "\n",
    "        # make pipeline\n",
    "        # use standardscaler for transformation\n",
    "        # use lasso for feature selection\n",
    "        # use random forest for prediction\n",
    "        param_grid = {\n",
    "            'selectfrommodel__estimator__alpha':[10**v for v in [-4,-3,-2,-1,0]], # too large alpha will produce a null model (all features are 0)\n",
    "            'randomforestregressor__max_features':['auto','sqrt','log2',0.16,0.32,0.64],\n",
    "            'randomforestregressor__max_depth':[2,4,8,16],\n",
    "            'randomforestregressor__min_samples_split':[2,4,8,16],\n",
    "            'randomforestregressor__min_samples_leaf':[1,2,4]\n",
    "        }\n",
    "        \n",
    "        clf1 = linear_model.Lasso(tol=1e-5,positive=True,random_state=0,max_iter=1000000)\n",
    "        clf2 = RandomForestRegressor(n_estimators=2000,random_state=0,oob_score=True)\n",
    "        pipe = make_pipeline(StandardScaler(), SelectFromModel(clf1, threshold=1e-5), clone(clf2))  \n",
    "        CV = GridSearchCV(pipe, param_grid, scoring='r2', cv=5, n_jobs=-1, verbose=0)\n",
    "        if use_weights:\n",
    "            CV.fit(xdata_train, ydata_train, selectfrommodel__sample_weight=weights, randomforestregressor__sample_weight=weights)   \n",
    "        else:\n",
    "            CV.fit(xdata_train, ydata_train)\n",
    "\n",
    "        print('Intrapolation, group %s, %s, best score and parameter combination = '%(group_to_exclude, scfa))\n",
    "        print(CV.best_score_)    \n",
    "        print(CV.best_params_)    \n",
    "        print('\\n')\n",
    "\n",
    "        # predict training set\n",
    "        ydata_train_predicted = CV.predict(xdata_train)\n",
    "        ydata_test_predicted = CV.predict(xdata_test)\n",
    "\n",
    "        for sample_, obs_, pred_ in zip(samples_to_keep, ydata_train, ydata_train_predicted):\n",
    "            day_ = df_meta.loc[sample_,'Day']\n",
    "            results.append(['intrapolation', scfa, group_to_exclude, 'train', sample_, day_, obs_, pred_])\n",
    "        for sample_, obs_, pred_ in zip(samples_to_exclude, ydata_test, ydata_test_predicted):\n",
    "            day_ = df_meta.loc[sample_,'Day']\n",
    "            results.append(['intrapolation', scfa, group_to_exclude, 'test', sample_, day_, obs_, pred_])\n",
    "\n",
    "if use_weights and plot_weights:\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation, vendor Beijing, Acetate, best score and parameter combination = \n",
      "0.15619169567068986\n",
      "{'randomforestregressor__max_depth': 8, 'randomforestregressor__max_features': 0.32, 'randomforestregressor__min_samples_leaf': 2, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.01}\n",
      "\n",
      "\n",
      "Extrapolation, vendor Beijing, Propionate, best score and parameter combination = \n",
      "0.4970656191458806\n",
      "{'randomforestregressor__max_depth': 16, 'randomforestregressor__max_features': 0.32, 'randomforestregressor__min_samples_leaf': 1, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.0001}\n",
      "\n",
      "\n",
      "Extrapolation, vendor Beijing, Butyrate, best score and parameter combination = \n",
      "0.2559657542269019\n",
      "{'randomforestregressor__max_depth': 4, 'randomforestregressor__max_features': 0.64, 'randomforestregressor__min_samples_leaf': 4, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.0001}\n",
      "\n",
      "\n",
      "Extrapolation, vendor Guangdong, Acetate, best score and parameter combination = \n",
      "0.07501339788650323\n",
      "{'randomforestregressor__max_depth': 4, 'randomforestregressor__max_features': 0.64, 'randomforestregressor__min_samples_leaf': 1, 'randomforestregressor__min_samples_split': 4, 'selectfrommodel__estimator__alpha': 0.0001}\n",
      "\n",
      "\n",
      "Extrapolation, vendor Guangdong, Propionate, best score and parameter combination = \n",
      "0.40296339446101087\n",
      "{'randomforestregressor__max_depth': 16, 'randomforestregressor__max_features': 0.64, 'randomforestregressor__min_samples_leaf': 1, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.001}\n",
      "\n",
      "\n",
      "Extrapolation, vendor Guangdong, Butyrate, best score and parameter combination = \n",
      "0.35094170637124467\n",
      "{'randomforestregressor__max_depth': 4, 'randomforestregressor__max_features': 0.64, 'randomforestregressor__min_samples_leaf': 2, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.001}\n",
      "\n",
      "\n",
      "Extrapolation, vendor Hunan, Acetate, best score and parameter combination = \n",
      "0.09906519655091403\n",
      "{'randomforestregressor__max_depth': 8, 'randomforestregressor__max_features': 0.64, 'randomforestregressor__min_samples_leaf': 2, 'randomforestregressor__min_samples_split': 16, 'selectfrommodel__estimator__alpha': 0.1}\n",
      "\n",
      "\n",
      "Extrapolation, vendor Hunan, Propionate, best score and parameter combination = \n",
      "0.42325116214597847\n",
      "{'randomforestregressor__max_depth': 8, 'randomforestregressor__max_features': 'auto', 'randomforestregressor__min_samples_leaf': 4, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.01}\n",
      "\n",
      "\n",
      "Extrapolation, vendor Hunan, Butyrate, best score and parameter combination = \n",
      "0.5073597582936348\n",
      "{'randomforestregressor__max_depth': 8, 'randomforestregressor__max_features': 'auto', 'randomforestregressor__min_samples_leaf': 4, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.1}\n",
      "\n",
      "\n",
      "Extrapolation, vendor Shanghai, Acetate, best score and parameter combination = \n",
      "0.12176075476447434\n",
      "{'randomforestregressor__max_depth': 4, 'randomforestregressor__max_features': 0.16, 'randomforestregressor__min_samples_leaf': 2, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.1}\n",
      "\n",
      "\n",
      "Extrapolation, vendor Shanghai, Propionate, best score and parameter combination = \n",
      "0.21022235154136837\n",
      "{'randomforestregressor__max_depth': 4, 'randomforestregressor__max_features': 0.64, 'randomforestregressor__min_samples_leaf': 4, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.01}\n",
      "\n",
      "\n",
      "Extrapolation, vendor Shanghai, Butyrate, best score and parameter combination = \n",
      "0.21268410704689203\n",
      "{'randomforestregressor__max_depth': 8, 'randomforestregressor__max_features': 0.32, 'randomforestregressor__min_samples_leaf': 2, 'randomforestregressor__min_samples_split': 2, 'selectfrommodel__estimator__alpha': 0.01}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if use_weights and plot_weights:\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(20,4), sharex=True, sharey=True)\n",
    "\n",
    "for k,vendor_to_exclude in enumerate(['Beijing','Guangdong','Hunan','Shanghai']):\n",
    "        \n",
    "    # split train/test data\n",
    "    mice_to_keep = list(set(df_meta[df_meta.Vendor!=vendor_to_exclude].MiceID))\n",
    "    samples_to_keep = list(set(df_meta[df_meta.MiceID.isin(mice_to_keep)].index))\n",
    "    mice_to_exclude = list(set(df_meta[df_meta.Vendor==vendor_to_exclude].MiceID))\n",
    "    samples_to_exclude = list(set(df_meta[df_meta.MiceID.isin(mice_to_exclude)].index))\n",
    "\n",
    "    # get weights of training sets\n",
    "    xdata_train = df_bac.loc[samples_to_keep]\n",
    "    xdata_test = df_bac.loc[samples_to_exclude]\n",
    "    if use_weights:\n",
    "        roc_auc[vendor_to_exclude], weights, df_ord = get_weights(X=xdata_test, Z=xdata_train, method='PCoA')\n",
    "        assert len(xdata_train)==len(weights)\n",
    "        print (\"ROC-AUC (%s): %2.2f\" % (vendor_to_exclude, roc_auc[vendor_to_exclude]))\n",
    "    \n",
    "        # show weights on train and test\n",
    "        if plot_weights:\n",
    "            # _ = sns.scatterplot(x='Axis1', y='Axis2', hue='is_z', size='size', data=df_ord, ax=ax[k])\n",
    "            df_ord_0 = df_ord.loc[df_ord.is_z==0]\n",
    "            _ = ax[k].scatter(df_ord_0['Axis1'], df_ord_0['Axis2'], marker='o', s=df_ord_0['size'], c='r', label='test')\n",
    "            df_ord_1 = df_ord.loc[df_ord.is_z==1]\n",
    "            _ = ax[k].scatter(df_ord_1['Axis1'], df_ord_1['Axis2'], marker='o', s=df_ord_1['size'], c='b', label='train')\n",
    "\n",
    "    xdata_train = np.asarray(xdata_train.values)\n",
    "    xdata_test = np.asarray(xdata_test.values)\n",
    "    \n",
    "    # run random forest regression with weights\n",
    "    for scfa in ['Acetate','Propionate','Butyrate']:                \n",
    "        ydata_train = np.asarray(df_scfa.loc[samples_to_keep, scfa])\n",
    "        ydata_test = np.asarray(df_scfa.loc[samples_to_exclude, scfa])\n",
    "\n",
    "        # make pipeline\n",
    "        # use lasso for feature selection\n",
    "        param_grid = {\n",
    "            'selectfrommodel__estimator__alpha':[10**v for v in [-4,-3,-2,-1,0]], # too large alpha will produce a null model (all features are 0)\n",
    "            'randomforestregressor__max_features':['auto','sqrt','log2',0.16,0.32,0.64],\n",
    "            'randomforestregressor__max_depth':[2,4,8,16],\n",
    "            'randomforestregressor__min_samples_split':[2,4,8,16],\n",
    "            'randomforestregressor__min_samples_leaf':[1,2,4]\n",
    "        }\n",
    "        \n",
    "        clf1 = linear_model.Lasso(tol=1e-5,positive=True,random_state=0,max_iter=1000000)\n",
    "        clf2 = RandomForestRegressor(n_estimators=2000,random_state=0,oob_score=True)\n",
    "        pipe = make_pipeline(StandardScaler(), SelectFromModel(clf1, threshold=1e-5), clone(clf2))  \n",
    "        CV = GridSearchCV(pipe, param_grid, scoring='r2', cv=5, n_jobs=-1, verbose=0)\n",
    "        if use_weights:\n",
    "            CV.fit(xdata_train, ydata_train, selectfrommodel__sample_weight=weights, randomforestregressor__sample_weight=weights)\n",
    "        else:\n",
    "            CV.fit(xdata_train, ydata_train)\n",
    "\n",
    "        print('Extrapolation, vendor %s, %s, best score and parameter combination = '%(vendor_to_exclude, scfa))\n",
    "        print(CV.best_score_)    \n",
    "        print(CV.best_params_)    \n",
    "        print('\\n')   \n",
    "\n",
    "        # predict training set\n",
    "        ydata_train_predicted = CV.predict(xdata_train)\n",
    "        ydata_test_predicted = CV.predict(xdata_test)\n",
    "\n",
    "        for sample_, obs_, pred_ in zip(samples_to_keep, ydata_train, ydata_train_predicted):\n",
    "            day_ = df_meta.loc[sample_,'Day']\n",
    "            results.append(['extrapolation', scfa, vendor_to_exclude, 'train', sample_, day_, obs_, pred_])\n",
    "        for sample_, obs_, pred_ in zip(samples_to_exclude, ydata_test, ydata_test_predicted):\n",
    "            day_ = df_meta.loc[sample_,'Day']\n",
    "            results.append(['extrapolation', scfa, vendor_to_exclude, 'test', sample_, day_, obs_, pred_])\n",
    "\n",
    "if use_weights and plot_weights:\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.418728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.508299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.476923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.625117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beijing</th>\n",
       "      <td>0.997691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guangdong</th>\n",
       "      <td>0.997855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hunan</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shanghai</th>\n",
       "      <td>0.999423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AUC\n",
       "A          0.418728\n",
       "B          0.508299\n",
       "C          0.476923\n",
       "D          0.625117\n",
       "Beijing    0.997691\n",
       "Guangdong  0.997855\n",
       "Hunan      1.000000\n",
       "Shanghai   0.999423"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_roc_w_weights = pd.DataFrame.from_dict(roc_auc, orient='index').rename({0:'AUC'}, axis=1)\n",
    "df_roc_w_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = pd.DataFrame(results, columns=['PerturbationType','SCFA','Permutation','PredictionType','SampleID','Day','ObservedValue','PredictedValue'])\n",
    "if use_weights:\n",
    "    df_prediction.to_csv('rf_prediction_predictor_genusonly_w_weights.csv')\n",
    "else:\n",
    "    df_prediction.to_csv('rf_prediction_predictor_genusonly_wo_weights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
