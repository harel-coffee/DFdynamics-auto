{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import make_pipeline                                           \n",
    "from sklearn.preprocessing import StandardScaler                                     \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.base import clone\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "%run -i '../../../../../../utils.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read meta data\n",
    "df_meta = pd.read_csv('../../../../../../our_data/meta_data.csv', index_col=0)\n",
    "df_meta = df_meta[df_meta.Diet=='Inulin'] # only for inulin group\n",
    "df_meta = df_meta[df_meta.Day != 0] # remove day 0\n",
    "\n",
    "# read SCFA data\n",
    "df_scfa = pd.read_csv('../../../../../../our_data/SCFA.csv', index_col=0)\n",
    "\n",
    "# read bacterial abundance\n",
    "df_bac = pd.read_csv('../../../../../../our_data/16S_absolute_abundance_species.csv', index_col=0)\n",
    "\n",
    "# find common samples\n",
    "common_samples = list(set(df_meta.index).intersection(df_scfa.index).intersection(df_bac.index))\n",
    "df_meta = df_meta.loc[common_samples]\n",
    "df_scfa = df_scfa.loc[common_samples]\n",
    "df_bac = df_bac.loc[common_samples]\n",
    "\n",
    "# add day to df_bac\n",
    "all_days = [1, 2, 3, 5, 8, 10, 13, 19, 25, 31]\n",
    "lines = []\n",
    "for sample in df_bac.index:\n",
    "    curr_line = [0]*len(all_days)\n",
    "    curr_line[all_days.index(df_meta.loc[sample,'Day'])] = 1\n",
    "    lines.append(curr_line)\n",
    "df_line = pd.DataFrame(lines, columns=['Day_'+str(day_) for day_ in all_days], index=df_bac.index)\n",
    "df_bac = pd.concat([df_bac, df_line], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_forest(params):\n",
    "    # get data\n",
    "    xdata_train = np.asarray(df_bac.values)\n",
    "    ydata_train = np.asarray(df_scfa[params['scfa']])\n",
    "\n",
    "    print(params)\n",
    "    # make pipeline\n",
    "    clf = RandomForestRegressor(n_estimators=params['n_trees'],\n",
    "                                max_features=params['max_features'],\n",
    "                                random_state=0, \n",
    "                                oob_score=True, \n",
    "                                max_depth=params['max_depth'],\n",
    "                                min_samples_split=params['min_samples_split'],\n",
    "                                min_samples_leaf=params['min_samples_leaf']\n",
    "                               )\n",
    "    pipe = make_pipeline(StandardScaler(), SelectFromModel(clf, max_features=params['n_features_to_select'], threshold=-np.inf), clone(clf))  \n",
    "    pipe.fit(xdata_train, ydata_train)\n",
    "    return list([params['scfa'],\n",
    "                 params['n_features_to_select'],\n",
    "                 params['n_trees'],\n",
    "                 params['max_features'],\n",
    "                 params['max_depth'],\n",
    "                 params['min_samples_split'],\n",
    "                 params['min_samples_leaf'],\n",
    "                 pipe[2].oob_score_])\n",
    "       \n",
    "#     # convert to panda\n",
    "#     df_tmp = pd.DataFrame.from_dict(params_exp, orient='index').T\n",
    "#     if df_oob is None:\n",
    "#         df_oob = deepcopy(df_tmp)\n",
    "#     else:\n",
    "#         df_oob = pd.concat([df_oob, df_tmp], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 72/17280 [00:11<30:12,  9.49it/s]"
     ]
    }
   ],
   "source": [
    "# parameters to test\n",
    "param_grid = {'scfa':['Acetate','Butyrate','Propionate'],\n",
    "              'n_features_to_select':[2,4,8,16,32],\n",
    "              'n_trees':[1024,2048,4096,8192],\n",
    "              'max_features':['auto','sqrt','log2',0.16,0.32,0.64],\n",
    "              'max_depth':[2,4,8,16],\n",
    "              'min_samples_split':[2,4,8,16],\n",
    "              'min_samples_leaf':[1,2,4]\n",
    "             }\n",
    "param_scores = Parallel(n_jobs=36)(delayed(run_random_forest)(params) for params in tqdm(ParameterGrid(param_grid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(param_scores, columns=['scfa','n_features_to_select','n_trees','max_features','max_depth','min_samples_split','min_samples_leaf','oob'])\n",
    "df_res.to_csv('param_grid_search_rf_oob.csv')\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run RF using best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for scfa in ['Acetate','Propionate','Butyrate']:\n",
    "    params = df_res[df_res.scfa==scfa].sort_values('oob', ascending=False).iloc[0]\n",
    "    for group_to_exclude in ['A','B','C','D']:\n",
    "        \n",
    "        # split train/test data\n",
    "        mice_to_keep = list(set(df_meta[df_meta.RandomizedGroup!=group_to_exclude].MiceID))\n",
    "        samples_to_keep = list(set(df_meta[df_meta.MiceID.isin(mice_to_keep)].index))\n",
    "        mice_to_exclude = list(set(df_meta[df_meta.RandomizedGroup==group_to_exclude].MiceID))\n",
    "        samples_to_exclude = list(set(df_meta[df_meta.MiceID.isin(mice_to_exclude)].index))\n",
    "        \n",
    "        # get X and Y data\n",
    "        xdata_train = np.asarray(df_bac.loc[samples_to_keep].values)\n",
    "        xdata_test = np.asarray(df_bac.loc[samples_to_exclude].values)\n",
    "        ydata_train = np.asarray(df_scfa.loc[samples_to_keep, scfa])\n",
    "        ydata_test = np.asarray(df_scfa.loc[samples_to_exclude, scfa])\n",
    "\n",
    "        # make pipeline\n",
    "        clf = RandomForestRegressor(n_estimators=params['n_trees'],\n",
    "                                    max_features=params['max_features'],\n",
    "                                    random_state=0, \n",
    "                                    oob_score=True, \n",
    "                                    max_depth=params['max_depth'],\n",
    "                                    min_samples_split=params['min_samples_split'],\n",
    "                                    min_samples_leaf=params['min_samples_leaf']\n",
    "                                   )\n",
    "        pipe = make_pipeline(StandardScaler(), SelectFromModel(clf, max_features=params['n_features_to_select'], threshold=-np.inf), clone(clf))  \n",
    "        pipe.fit(xdata_train, ydata_train)\n",
    "        \n",
    "        # predict training set\n",
    "        ydata_train_predicted = pipe.predict(xdata_train)\n",
    "        ydata_test_predicted = pipe.predict(xdata_test)\n",
    "\n",
    "        for sample_, obs_, pred_ in zip(samples_to_keep, ydata_train, ydata_train_predicted):\n",
    "            day_ = df_meta.loc[sample_,'Day']\n",
    "            lines.append([scfa, group_to_exclude, 'train', sample_, day_, obs_, pred_])\n",
    "        for sample_, obs_, pred_ in zip(samples_to_exclude, ydata_test, ydata_test_predicted):\n",
    "            day_ = df_meta.loc[sample_,'Day']\n",
    "            lines.append([scfa, group_to_exclude, 'test', sample_, day_, obs_, pred_])\n",
    "\n",
    "df_pred_intra = pd.DataFrame(lines, columns=['SCFA','Permutation','PredictionType','SampleID','Day','ObservedValue','PredictedValue'])\n",
    "df_pred_intra.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for scfa in ['Acetate','Propionate','Butyrate']:\n",
    "    params = df_res[df_res.scfa==scfa].sort_values('oob', ascending=False).iloc[0]\n",
    "    for vendor_to_exclude in ['Beijing','Guangdong','Hunan','Shanghai']:\n",
    "        \n",
    "        # split train/test data\n",
    "        mice_to_keep = list(set(df_meta[df_meta.Vendor!=vendor_to_exclude].MiceID))\n",
    "        samples_to_keep = list(set(df_meta[df_meta.MiceID.isin(mice_to_keep)].index))\n",
    "        mice_to_exclude = list(set(df_meta[df_meta.Vendor==vendor_to_exclude].MiceID))\n",
    "        samples_to_exclude = list(set(df_meta[df_meta.MiceID.isin(mice_to_exclude)].index))\n",
    "        \n",
    "        # get X and Y data\n",
    "        xdata_train = np.asarray(df_bac.loc[samples_to_keep].values)\n",
    "        xdata_test = np.asarray(df_bac.loc[samples_to_exclude].values)\n",
    "        ydata_train = np.asarray(df_scfa.loc[samples_to_keep, scfa])\n",
    "        ydata_test = np.asarray(df_scfa.loc[samples_to_exclude, scfa])\n",
    "\n",
    "        # make pipeline\n",
    "        clf = RandomForestRegressor(n_estimators=params['n_trees'],\n",
    "                                    max_features=params['max_features'],\n",
    "                                    random_state=0, \n",
    "                                    oob_score=True, \n",
    "                                    max_depth=params['max_depth'],\n",
    "                                    min_samples_split=params['min_samples_split'],\n",
    "                                    min_samples_leaf=params['min_samples_leaf']\n",
    "                                   )\n",
    "        pipe = make_pipeline(StandardScaler(), SelectFromModel(clf, max_features=params['n_features_to_select'], threshold=-np.inf), clone(clf))  \n",
    "        pipe.fit(xdata_train, ydata_train)\n",
    "        \n",
    "        # predict training set\n",
    "        ydata_train_predicted = pipe.predict(xdata_train)\n",
    "        ydata_test_predicted = pipe.predict(xdata_test)\n",
    "\n",
    "        for sample_, obs_, pred_ in zip(samples_to_keep, ydata_train, ydata_train_predicted):\n",
    "            day_ = df_meta.loc[sample_,'Day']\n",
    "            lines.append([scfa, vendor_to_exclude, 'train', sample_, day_, obs_, pred_])\n",
    "        for sample_, obs_, pred_ in zip(samples_to_exclude, ydata_test, ydata_test_predicted):\n",
    "            day_ = df_meta.loc[sample_,'Day']\n",
    "            lines.append([scfa, vendor_to_exclude, 'test', sample_, day_, obs_, pred_])\n",
    "\n",
    "df_pred_extra = pd.DataFrame(lines, columns=['SCFA','Permutation','PredictionType','SampleID','Day','ObservedValue','PredictedValue'])\n",
    "df_pred_extra.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,12), sharex=True, sharey=True)\n",
    "\n",
    "for k,ptype in enumerate(['train','test']):\n",
    "    # intrapolation\n",
    "    df_intra_tmp = df_pred_intra[df_pred_intra['PredictionType']==ptype]\n",
    "    _ = sns.scatterplot(x='ObservedValue', y='PredictedValue', hue='SCFA', size='Day', data=df_intra_tmp, ax=ax[k,0])\n",
    "    # extrapolation\n",
    "    df_extra_tmp = df_pred_extra[df_pred_extra['PredictionType']==ptype]\n",
    "    _ = sns.scatterplot(x='ObservedValue', y='PredictedValue', hue='SCFA', size='Day', data=df_extra_tmp, ax=ax[k,1])\n",
    "    \n",
    "    # calculate R2\n",
    "    r2_intra = []\n",
    "    r2_extra = []\n",
    "    for scfa in ['Acetate','Butyrate','Propionate']:\n",
    "        # intrapolation\n",
    "        df_intra_tmp2 = df_intra_tmp[df_intra_tmp.SCFA==scfa]\n",
    "        r2_intra.append(r2_score(df_intra_tmp2.ObservedValue, df_intra_tmp2.PredictedValue))\n",
    "        # extrapolation\n",
    "        df_extra_tmp2 = df_extra_tmp[df_extra_tmp.SCFA==scfa]\n",
    "        r2_extra.append(r2_score(df_extra_tmp2.ObservedValue, df_extra_tmp2.PredictedValue))\n",
    "   \n",
    "    # add title\n",
    "    _ = ax[k,0].set_title('R2 = %2.2f (%s), %2.2f (%s), %2.2f (%s)'%(r2_intra[0], 'Acetate', r2_intra[1], 'Butyrate', r2_intra[2], 'Propionate'))\n",
    "    _ = ax[k,1].set_title('R2 = %2.2f (%s), %2.2f (%s), %2.2f (%s)'%(r2_extra[0], 'Acetate', r2_extra[1], 'Butyrate', r2_extra[2], 'Propionate'))\n",
    "    \n",
    "    # set xlim and ylim\n",
    "    _ = ax[0,k].set_xlim([0,45])\n",
    "    _ = ax[1,k].set_xlim([0,45])\n",
    "    _ = ax[0,k].set_ylim([0,45])\n",
    "    _ = ax[1,k].set_ylim([0,45])\n",
    "    _ = ax[0,k].plot([0,45],[0,45],'k--')\n",
    "    _ = ax[1,k].plot([0,45],[0,45],'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
