{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import make_pipeline                                           \n",
    "from sklearn.preprocessing import StandardScaler                                     \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.base import clone\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import distance\n",
    "from skbio.stats.ordination import pcoa\n",
    "import umap\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import linear_model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read meta data\n",
    "df_meta = pd.read_csv('../../our_data/meta_data.csv', index_col=0)\n",
    "df_meta = df_meta[df_meta.Diet=='Inulin']\n",
    "df_meta = df_meta[df_meta.Day != 0] # remove day 0\n",
    "\n",
    "# read SCFA data\n",
    "df_scfa = pd.read_csv('../../our_data/SCFA.csv', index_col=0)\n",
    "\n",
    "# read bacterial abundance (species level)\n",
    "df_bac = pd.read_csv('../../our_data/16S_absolute_abundance_species.csv', index_col=0)\n",
    "\n",
    "# find common samples\n",
    "common_samples = list(set(df_meta.index).intersection(df_scfa.index).intersection(df_bac.index))\n",
    "df_meta = df_meta.loc[common_samples]\n",
    "df_scfa = df_scfa.loc[common_samples]\n",
    "df_bac = df_bac.loc[common_samples]\n",
    "\n",
    "# remove species that are constant across all samples\n",
    "df_bac = df_bac[list(df_bac.std()[df_bac.std()>0].index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 49 candidates, totalling 245 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    1.5s\n"
     ]
    }
   ],
   "source": [
    "for k,group_to_exclude in enumerate(['A','B','C','D']):\n",
    "\n",
    "    # split train/test data\n",
    "    mice_to_keep = list(set(df_meta[df_meta.RandomizedGroup!=group_to_exclude].MiceID))\n",
    "    samples_to_keep = list(set(df_meta[df_meta.MiceID.isin(mice_to_keep)].index))\n",
    "    mice_to_exclude = list(set(df_meta[df_meta.RandomizedGroup==group_to_exclude].MiceID))\n",
    "    samples_to_exclude = list(set(df_meta[df_meta.MiceID.isin(mice_to_exclude)].index))\n",
    "\n",
    "    # get weights of training sets\n",
    "    xdata_train = np.asarray(df_bac.loc[samples_to_keep].values)\n",
    "    xdata_test = np.asarray(df_bac.loc[samples_to_exclude].values)\n",
    "    \n",
    "    # run random forest regression with weights\n",
    "    for scfa in ['Acetate','Propionate','Butyrate']:                \n",
    "        ydata_train = np.asarray(df_scfa.loc[samples_to_keep, scfa])\n",
    "        ydata_test = np.asarray(df_scfa.loc[samples_to_exclude, scfa])\n",
    "        param_grid = {\"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                      \"l1_ratio\": [.01, .1, .3, .5, .7, .9, .99]}\n",
    "        \n",
    "        clf = ElasticNet(tol=1e-5,random_state=0,max_iter=1000000)\n",
    "        CV = GridSearchCV(clf, param_grid, scoring='r2', cv=5, n_jobs=-1, verbose=2)\n",
    "        CV.fit(xdata_train, ydata_train)\n",
    "\n",
    "        print('Intrapolation, group %s, %s, best score and parameter combination = '%(group_to_exclude, scfa))\n",
    "        print(CV.best_score_)    \n",
    "        print(CV.best_params_)    \n",
    "        print('\\n')\n",
    "\n",
    "        # predict training set\n",
    "        ydata_train_predicted = CV.predict(xdata_train)\n",
    "        ydata_test_predicted = CV.predict(xdata_test)\n",
    "\n",
    "        for sample_, obs_, pred_ in zip(samples_to_keep, ydata_train, ydata_train_predicted):\n",
    "            day_ = df_meta.loc[sample_,'Day']\n",
    "            results.append(['intrapolation', scfa, group_to_exclude, 'train', sample_, day_, obs_, pred_])\n",
    "        for sample_, obs_, pred_ in zip(samples_to_exclude, ydata_test, ydata_test_predicted):\n",
    "            day_ = df_meta.loc[sample_,'Day']\n",
    "            results.append(['intrapolation', scfa, group_to_exclude, 'test', sample_, day_, obs_, pred_])\n",
    "\n",
    "if use_weights and plot_weights:\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,vendor_to_exclude in enumerate(['Beijing','Guangdong','Hunan','Shanghai']):\n",
    "        \n",
    "    # split train/test data\n",
    "    mice_to_keep = list(set(df_meta[df_meta.Vendor!=vendor_to_exclude].MiceID))\n",
    "    samples_to_keep = list(set(df_meta[df_meta.MiceID.isin(mice_to_keep)].index))\n",
    "    mice_to_exclude = list(set(df_meta[df_meta.Vendor==vendor_to_exclude].MiceID))\n",
    "    samples_to_exclude = list(set(df_meta[df_meta.MiceID.isin(mice_to_exclude)].index))\n",
    "\n",
    "    # get weights of training sets\n",
    "    xdata_train = np.asarray(df_bac.loc[samples_to_keep].values)\n",
    "    xdata_test = np.asarray(df_bac.loc[samples_to_exclude].values)\n",
    "    \n",
    "    # run random forest regression with weights\n",
    "    for scfa in ['Acetate','Propionate','Butyrate']:                \n",
    "        ydata_train = np.asarray(df_scfa.loc[samples_to_keep, scfa])\n",
    "        ydata_test = np.asarray(df_scfa.loc[samples_to_exclude, scfa])\n",
    "        param_grid = {\"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                      \"l1_ratio\": [.01, .1, .3, .5, .7, .9, .99]}\n",
    "        \n",
    "        clf = ElasticNet(tol=1e-5,random_state=0,max_iter=1000000)\n",
    "        CV = GridSearchCV(clf, param_grid, scoring='r2', cv=5, n_jobs=-1, verbose=2)\n",
    "        CV.fit(xdata_train, ydata_train)\n",
    "        print('Extrapolation, vendor %s, %s, best score and parameter combination = '%(vendor_to_exclude, scfa))\n",
    "        print(CV.best_score_)    \n",
    "        print(CV.best_params_)    \n",
    "        print('\\n')   \n",
    "\n",
    "        # predict training set\n",
    "        ydata_train_predicted = CV.predict(xdata_train)\n",
    "        ydata_test_predicted = CV.predict(xdata_test)\n",
    "\n",
    "        for sample_, obs_, pred_ in zip(samples_to_keep, ydata_train, ydata_train_predicted):\n",
    "            day_ = df_meta.loc[sample_,'Day']\n",
    "            results.append(['extrapolation', scfa, vendor_to_exclude, 'train', sample_, day_, obs_, pred_])\n",
    "        for sample_, obs_, pred_ in zip(samples_to_exclude, ydata_test, ydata_test_predicted):\n",
    "            day_ = df_meta.loc[sample_,'Day']\n",
    "            results.append(['extrapolation', scfa, vendor_to_exclude, 'test', sample_, day_, obs_, pred_])\n",
    "\n",
    "if use_weights and plot_weights:\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = pd.DataFrame(results, columns=['PerturbationType','SCFA','Permutation','PredictionType','SampleID','Day','ObservedValue','PredictedValue'])\n",
    "df_prediction.to_csv('rf_prediction_predictor_speciesonly_elasticnet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
